{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = 'C:/Users/kimms/OneDrive/바탕 화면/소캡디/' # input only here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting other directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = dir_+'2. data/'\n",
    "processed_data_dir = dir_+'2. data/processed/'\n",
    "log_dir = dir_+'4. logs/'\n",
    "model_dir = dir_+'5. models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver, KKK = 'priv', 0\n",
    "STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## SEED 설정 ####################################\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Target Store에 해당하는 Data 로드############################\n",
    "\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    df = df[df['d']>=START_TRAIN]\n",
    "    \n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2\n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3\n",
    "\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle(processed_data_dir+'test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "#################### 전처리를 위한 (이동평균선) 데이터 전처리 코드 ################################33\n",
    "\n",
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 튜닝 후 refit 이용해 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Default model parameter설정 (기준값) ##########################\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb_params = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### 전처리 변수 생성 및 데이터 불러오기 #########################\n",
    "\n",
    "VER = 1                          \n",
    "SEED = 42                        \n",
    "seed_everything(SEED)            \n",
    "N_CORES = psutil.cpu_count()     \n",
    "\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            \n",
    "START_TRAIN = 0                  \n",
    "END_TRAIN   = 1941 - 28*KKK      \n",
    "P_HORIZON   = 28                 \n",
    "USE_AUX     = False             \n",
    "\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "ORIGINAL = raw_data_dir\n",
    "BASE     = processed_data_dir+'grid_part_1.pkl'\n",
    "PRICE    = processed_data_dir+'grid_part_2.pkl'\n",
    "CALENDAR = processed_data_dir+'grid_part_3.pkl'\n",
    "LAGS     = processed_data_dir+'lags_df_28.pkl'\n",
    "MEAN_ENC = processed_data_dir+'mean_encoding_df.pkl'\n",
    "\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든 Store에 대해서 하이퍼 파라미터 튜닝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_search_space = {'num_leaves': hp.quniform('num_leaves',2**9,2**10,5),\n",
    "                        'max_depth': hp.quniform('max_depth',80,140,1),\n",
    "                     'min_data_in_leaf': hp.quniform('min_data_in_leaf',2**10,2**11,10),\n",
    "                        'subsample':hp.uniform('subsample', 0.5,1),\n",
    "                        'learning_rate': hp.uniform('learning_rate',0.01,0.2)}\n",
    "\n",
    "def objective_func(search_space):\n",
    "    lgbm_clf = LGBMRegressor(n_estimators = 1000,\n",
    "                              boosting_type ='gbdt',\n",
    "                              objective='tweedie',\n",
    "                              tweedie_variance_power=1.1,\n",
    "                              metric='rmse',\n",
    "                              boost_from_average=False, \n",
    "                              feature_fraction=0.5,\n",
    "                              max_bin=100,\n",
    "                              num_leaves = int(search_space['num_leaves']),\n",
    "                              max_depth= int(search_space['max_depth']),\n",
    "                              min_data_in_leaf = int(search_space['min_data_in_leaf']),\n",
    "                              subsample = search_space['subsample'],\n",
    "                              learning_rate = search_space['learning_rate'],\n",
    "                              n_jobs = -1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    lgbm_clf.fit(x_train,y_train)\n",
    "    pred = lgbm_clf.predict(latest_x_valid)\n",
    "    rms = sqrt(mean_squared_error(latest_y_valid, pred))\n",
    "    print(\"valid: \",rms)\n",
    "    \n",
    "    return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Teacher_Store in tqdm(T_STORES_IDS):\n",
    "    ########################### Train Teacher Models\n",
    "    #################################################################################\n",
    "    print('Teacher Train', Teacher_Store)\n",
    "    store_id = Teacher_Store\n",
    "    best = parameter_dict[Teacher_Store]\n",
    "    lgb_params = {  \n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': round(best['subsample'],5),\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': round(best['learning_rate'],5),\n",
    "                    'num_leaves': int(best['num_leaves']),\n",
    "                    'min_data_in_leaf': int(best['min_data_in_leaf']),\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1000,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                    'max_depth':int(best['max_depth'])\n",
    "                } \n",
    "\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = (grid_df['d']>(END_TRAIN-100)) & (grid_df['d'] <= END_TRAIN+P_HORIZON)\n",
    "\n",
    "    # 최근 약 3년의 데이터만 사용 \n",
    "    latest_1y = int(len(train_mask)/5)*3\n",
    "\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns].iloc[-latest_1y:], \n",
    "                       label=grid_df[train_mask][TARGET].iloc[-latest_1y:])\n",
    "\n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])\n",
    "\n",
    "\n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "\n",
    "    d_sales = grid_df[['d','sales']]\n",
    "    substitute = d_sales['sales'].values\n",
    "    substitute[(d_sales['d'] > END_TRAIN)] = np.nan\n",
    "    grid_df['sales'] = substitute\n",
    "\n",
    "    grid_df.to_pickle(processed_data_dir+'test_'+store_id+'.pkl')\n",
    "    del grid_df, d_sales, substitute\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "\n",
    "\n",
    "    display(pd.DataFrame({'name':estimator.feature_name(),\n",
    "                          'imp':estimator.feature_importance()}).sort_values('imp',ascending=False).head(25))\n",
    "\n",
    "\n",
    "    model_name = model_dir+'lgb_model_3y_T_'+store_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Teacher_Store in tqdm(STORES_IDS):\n",
    "    ########################### Train Teacher Models\n",
    "    #################################################################################\n",
    "    print('Teacher Train', Teacher_Store)\n",
    "    store_id = Teacher_Store\n",
    "    best = oney_parameter_dict[Teacher_Store]\n",
    "    lgb_params = {  \n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': round(best['subsample'],5),\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': round(best['learning_rate'],5),\n",
    "                    'num_leaves': int(best['num_leaves']),\n",
    "                    'min_data_in_leaf': int(best['min_data_in_leaf']),\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1000,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                    'max_depth':int(best['max_depth'])\n",
    "                } \n",
    "\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = (grid_df['d']>(END_TRAIN-100)) & (grid_df['d'] <= END_TRAIN+P_HORIZON)\n",
    "\n",
    "    # 최근 약 3년의 데이터만 사용 \n",
    "    latest_1y = int(len(train_mask)/5)*3\n",
    "\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns].iloc[-latest_1y:], \n",
    "                       label=grid_df[train_mask][TARGET].iloc[-latest_1y:])\n",
    "\n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])\n",
    "\n",
    "\n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "\n",
    "    d_sales = grid_df[['d','sales']]\n",
    "    substitute = d_sales['sales'].values\n",
    "    substitute[(d_sales['d'] > END_TRAIN)] = np.nan\n",
    "    grid_df['sales'] = substitute\n",
    "\n",
    "    grid_df.to_pickle(processed_data_dir+'test_'+store_id+'.pkl')\n",
    "    del grid_df, d_sales, substitute\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "\n",
    "\n",
    "    display(pd.DataFrame({'name':estimator.feature_name(),\n",
    "                          'imp':estimator.feature_importance()}).sort_values('imp',ascending=False).head(25))\n",
    "\n",
    "\n",
    "    model_name = model_dir+'lgb_model_1y_hy_'+store_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CA 1부터 4까지 각각 Teacher model로 잡고 각 경우에서 나머지 9개 store에 대해서 refit을 사용해서 재학습 진행 및 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_STORES_IDS = ['CA_1','CA_2','CA_3','CA_4']\n",
    "S_STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_store_id in tqdm(T_STORES_IDS):\n",
    "    Teacher_Store = t_store_id\n",
    "    print(\"Teacher store: \", Teacher_Store)\n",
    "    \n",
    "    for store_id in tqdm(S_STORES_IDS):\n",
    "        \n",
    "        # 학습된 Teacher model\n",
    "        model_path = model_dir+'lgb_model_3y_T_'+Teacher_Store+'_v'+str(VER)+'.bin' \n",
    "        if USE_AUX:\n",
    "            model_path = AUX_MODELS + model_path\n",
    "\n",
    "        T_model = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        if t_store_id == store_id:\n",
    "            # teacher model에 대해서는 재학습이 아닌 그대로 모델을 사용 \n",
    "            model_name = model_dir+'lgb_model_3y_'+Teacher_Store+'_'+store_id+'_v'+str(VER)+'.bin'\n",
    "            pickle.dump(T_model, open(model_name, 'wb'))\n",
    "            continue\n",
    "            \n",
    "        print('Train', store_id)\n",
    "        grid_df, features_columns = get_data_by_store(store_id)\n",
    "\n",
    "        train_mask = grid_df['d']<=END_TRAIN\n",
    "        valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "        preds_mask = (grid_df['d']>(END_TRAIN-100)) & (grid_df['d'] <= END_TRAIN+P_HORIZON)\n",
    "\n",
    "        train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                           label=grid_df[train_mask][TARGET])\n",
    "\n",
    "        valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                           label=grid_df[valid_mask][TARGET])\n",
    "\n",
    "        # 학습 데이터 정의 \n",
    "        x_train = grid_df[train_mask][features_columns]\n",
    "        y_train = grid_df[train_mask][TARGET]\n",
    "\n",
    "        # 최근 약 1년의 데이터만 사용 \n",
    "        latest_1y = int(len(train_mask)/5)\n",
    "\n",
    "        x_train = x_train.iloc[-latest_1y:]\n",
    "        y_train = y_train.iloc[-latest_1y:]\n",
    "\n",
    "\n",
    "        # valid 데이터 정의 \n",
    "        full_x_valid = grid_df[valid_mask][features_columns]\n",
    "        full_y_valid = grid_df[valid_mask][TARGET]\n",
    "\n",
    "        latest_x_valid = full_x_valid.iloc[-latest_1y:]\n",
    "        latest_y_valid = full_y_valid.iloc[-latest_1y:]\n",
    "\n",
    "        grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "        keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "        grid_df = grid_df[keep_cols]\n",
    "\n",
    "        d_sales = grid_df[['d','sales']]\n",
    "        substitute = d_sales['sales'].values\n",
    "        substitute[(d_sales['d'] > END_TRAIN)] = np.nan\n",
    "        grid_df['sales'] = substitute\n",
    "\n",
    "        grid_df.to_pickle(processed_data_dir+'test_'+store_id+'.pkl')\n",
    "        del grid_df, d_sales, substitute\n",
    "\n",
    "\n",
    "        seed_everything(SEED)\n",
    "        S_model = T_model.refit(data=x_train, label=y_train, decay_rate=0)\n",
    "        y_pred = S_model.predict(full_x_valid)\n",
    "        score = mean_squared_error(full_y_valid, y_pred)**0.5\n",
    "        print(\"Full period RMSE: \",score)\n",
    "\n",
    "        y_pred = S_model.predict(latest_x_valid)\n",
    "        score = mean_squared_error(latest_y_valid, y_pred)**0.5\n",
    "        print(\"latest period RMSE: \",score)\n",
    "\n",
    "        display(pd.DataFrame({'name':S_model.feature_name(),\n",
    "                              'imp':S_model.feature_importance()}).sort_values('imp',ascending=False).head(25))\n",
    "\n",
    "\n",
    "        model_name = model_dir+'lgb_model_3y_'+Teacher_Store+'_'+store_id+'_v'+str(VER)+'.bin'\n",
    "        pickle.dump(S_model, open(model_name, 'wb'))\n",
    "\n",
    "\n",
    "        del train_data, valid_data, T_model\n",
    "        gc.collect()\n",
    "\n",
    "        MODEL_FEATURES = features_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TX 1부터 3까지 각각 Teacher model로 잡고 각 경우에서 나머지 9개 store에 대해서 refit을 사용해서 재학습 진행 및 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_STORES_IDS = ['TX_1','TX_2','TX_3']\n",
    "S_STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_store_id in tqdm(T_STORES_IDS):\n",
    "    Teacher_Store = t_store_id\n",
    "    print(\"Teacher store: \", Teacher_Store)\n",
    "    \n",
    "    for store_id in tqdm(S_STORES_IDS):\n",
    "        \n",
    "        # 학습된 Teacher model\n",
    "        model_path = model_dir+'lgb_model_3y_T_'+Teacher_Store+'_v'+str(VER)+'.bin' \n",
    "        if USE_AUX:\n",
    "            model_path = AUX_MODELS + model_path\n",
    "\n",
    "        T_model = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        if t_store_id == store_id:\n",
    "            # teacher model에 대해서는 재학습이 아닌 그대로 모델을 사용 \n",
    "            model_name = model_dir+'lgb_model_3y_'+Teacher_Store+'_'+store_id+'_v'+str(VER)+'.bin'\n",
    "            pickle.dump(T_model, open(model_name, 'wb'))\n",
    "            continue\n",
    "            \n",
    "        print('Train', store_id)\n",
    "        grid_df, features_columns = get_data_by_store(store_id)\n",
    "\n",
    "        train_mask = grid_df['d']<=END_TRAIN\n",
    "        valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "        preds_mask = (grid_df['d']>(END_TRAIN-100)) & (grid_df['d'] <= END_TRAIN+P_HORIZON)\n",
    "\n",
    "        train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                           label=grid_df[train_mask][TARGET])\n",
    "\n",
    "        valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                           label=grid_df[valid_mask][TARGET])\n",
    "\n",
    "        # 학습 데이터 정의 \n",
    "        x_train = grid_df[train_mask][features_columns]\n",
    "        y_train = grid_df[train_mask][TARGET]\n",
    "\n",
    "        # 최근 약 1년의 데이터만 사용 \n",
    "        latest_1y = int(len(train_mask)/5)\n",
    "\n",
    "        x_train = x_train.iloc[-latest_1y:]\n",
    "        y_train = y_train.iloc[-latest_1y:]\n",
    "\n",
    "\n",
    "        # valid 데이터 정의 \n",
    "        full_x_valid = grid_df[valid_mask][features_columns]\n",
    "        full_y_valid = grid_df[valid_mask][TARGET]\n",
    "\n",
    "        latest_x_valid = full_x_valid.iloc[-latest_1y:]\n",
    "        latest_y_valid = full_y_valid.iloc[-latest_1y:]\n",
    "\n",
    "        grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "        keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "        grid_df = grid_df[keep_cols]\n",
    "\n",
    "        d_sales = grid_df[['d','sales']]\n",
    "        substitute = d_sales['sales'].values\n",
    "        substitute[(d_sales['d'] > END_TRAIN)] = np.nan\n",
    "        grid_df['sales'] = substitute\n",
    "\n",
    "        grid_df.to_pickle(processed_data_dir+'test_'+store_id+'.pkl')\n",
    "        del grid_df, d_sales, substitute\n",
    "\n",
    "\n",
    "        seed_everything(SEED)\n",
    "        S_model = T_model.refit(data=x_train, label=y_train, decay_rate=0)\n",
    "        y_pred = S_model.predict(full_x_valid)\n",
    "        score = mean_squared_error(full_y_valid, y_pred)**0.5\n",
    "        print(\"Full period RMSE: \",score)\n",
    "\n",
    "        y_pred = S_model.predict(latest_x_valid)\n",
    "        score = mean_squared_error(latest_y_valid, y_pred)**0.5\n",
    "        print(\"latest period RMSE: \",score)\n",
    "\n",
    "        display(pd.DataFrame({'name':S_model.feature_name(),\n",
    "                              'imp':S_model.feature_importance()}).sort_values('imp',ascending=False).head(25))\n",
    "\n",
    "\n",
    "        model_name = model_dir+'lgb_model_3y_'+Teacher_Store+'_'+store_id+'_v'+str(VER)+'.bin'\n",
    "        pickle.dump(S_model, open(model_name, 'wb'))\n",
    "\n",
    "\n",
    "        del train_data, valid_data, T_model\n",
    "        gc.collect()\n",
    "\n",
    "        MODEL_FEATURES = features_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    WI 1부터 3까지 각각 Teacher model로 잡고 각 경우에서 나머지 9개 store에 대해서 refit을 사용해서 재학습 진행 및 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_STORES_IDS = ['WI_1','WI_2','WI_3']\n",
    "S_STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_store_id in tqdm(T_STORES_IDS):\n",
    "    Teacher_Store = t_store_id\n",
    "    print(\"Teacher store: \", Teacher_Store)\n",
    "    \n",
    "    for store_id in tqdm(S_STORES_IDS):\n",
    "        \n",
    "        # 학습된 Teacher model\n",
    "        model_path = model_dir+'lgb_model_3y_T_'+Teacher_Store+'_v'+str(VER)+'.bin' \n",
    "        if USE_AUX:\n",
    "            model_path = AUX_MODELS + model_path\n",
    "\n",
    "        T_model = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        if t_store_id == store_id:\n",
    "            # teacher model에 대해서는 재학습이 아닌 그대로 모델을 사용 \n",
    "            model_name = model_dir+'lgb_model_3y_'+Teacher_Store+'_'+store_id+'_v'+str(VER)+'.bin'\n",
    "            pickle.dump(T_model, open(model_name, 'wb'))\n",
    "            continue\n",
    "            \n",
    "        print('Train', store_id)\n",
    "        grid_df, features_columns = get_data_by_store(store_id)\n",
    "\n",
    "        train_mask = grid_df['d']<=END_TRAIN\n",
    "        valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "        preds_mask = (grid_df['d']>(END_TRAIN-100)) & (grid_df['d'] <= END_TRAIN+P_HORIZON)\n",
    "\n",
    "        train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                           label=grid_df[train_mask][TARGET])\n",
    "\n",
    "        valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                           label=grid_df[valid_mask][TARGET])\n",
    "\n",
    "        # 학습 데이터 정의 \n",
    "        x_train = grid_df[train_mask][features_columns]\n",
    "        y_train = grid_df[train_mask][TARGET]\n",
    "\n",
    "        # 최근 약 1년의 데이터만 사용 \n",
    "        latest_1y = int(len(train_mask)/5)\n",
    "\n",
    "        x_train = x_train.iloc[-latest_1y:]\n",
    "        y_train = y_train.iloc[-latest_1y:]\n",
    "\n",
    "\n",
    "        # valid 데이터 정의 \n",
    "        full_x_valid = grid_df[valid_mask][features_columns]\n",
    "        full_y_valid = grid_df[valid_mask][TARGET]\n",
    "\n",
    "        latest_x_valid = full_x_valid.iloc[-latest_1y:]\n",
    "        latest_y_valid = full_y_valid.iloc[-latest_1y:]\n",
    "\n",
    "        grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "        keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "        grid_df = grid_df[keep_cols]\n",
    "\n",
    "        d_sales = grid_df[['d','sales']]\n",
    "        substitute = d_sales['sales'].values\n",
    "        substitute[(d_sales['d'] > END_TRAIN)] = np.nan\n",
    "        grid_df['sales'] = substitute\n",
    "\n",
    "        grid_df.to_pickle(processed_data_dir+'test_'+store_id+'.pkl')\n",
    "        del grid_df, d_sales, substitute\n",
    "\n",
    "\n",
    "        seed_everything(SEED)\n",
    "        S_model = T_model.refit(data=x_train, label=y_train, decay_rate=0)\n",
    "        y_pred = S_model.predict(full_x_valid)\n",
    "        score = mean_squared_error(full_y_valid, y_pred)**0.5\n",
    "        print(\"Full period RMSE: \",score)\n",
    "\n",
    "        y_pred = S_model.predict(latest_x_valid)\n",
    "        score = mean_squared_error(latest_y_valid, y_pred)**0.5\n",
    "        print(\"latest period RMSE: \",score)\n",
    "\n",
    "        display(pd.DataFrame({'name':S_model.feature_name(),\n",
    "                              'imp':S_model.feature_importance()}).sort_values('imp',ascending=False).head(25))\n",
    "\n",
    "\n",
    "        model_name = model_dir+'lgb_model_3y_'+Teacher_Store+'_'+store_id+'_v'+str(VER)+'.bin'\n",
    "        pickle.dump(S_model, open(model_name, 'wb'))\n",
    "\n",
    "\n",
    "        del train_data, valid_data, T_model\n",
    "        gc.collect()\n",
    "\n",
    "        MODEL_FEATURES = features_columns\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
